{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from transformers import AutoTokenizer , TFAutoModel \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset \n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Q_A_signs(data) : \n",
    "    data['question1'] = '<Q>' + data['question1']\n",
    "    data['question2'] = '<A>' + data['question2']\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = add_Q_A_signs(train_data)\n",
    "test_data = add_Q_A_signs(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data[[ 'question1' , 'question2'  , 'is_duplicate']].iloc[:10000]) \n",
    "eval_dataset = Dataset.from_pandas(train_data[['question1' , 'question2'  , 'is_duplicate']].iloc[10000:10500])\n",
    "test_dataset = Dataset.from_pandas(test_data[['question1' , 'question2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tock_id = 'clips/mfaq'\n",
    "tock = AutoTokenizer.from_pretrained(tock_id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tock(dataset[\"question1\"], dataset['question2'],  padding='max_length', truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_dataset , batched = True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_dataset , batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "    columns = ['attention_mask' ,  'input_ids'  ] ,\n",
    "    label_cols = ['is_duplicate'] , \n",
    "    shuffle = True ,\n",
    "    collate_fn =  data_collator  , \n",
    "    batch_size = 16,\n",
    ")\n",
    "tf_eval_dataset = tokenized_eval_dataset.to_tf_dataset(\n",
    "    columns = ['attention_mask' ,  'input_ids'  ] ,\n",
    "    label_cols = ['is_duplicate'] , \n",
    "    shuffle = True ,\n",
    "    collate_fn =  data_collator  , \n",
    "    batch_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'clips/mfaq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(tf.keras.Model) : \n",
    "    def __init__(self ) : \n",
    "        super(MODEL , self).__init__()\n",
    "        self.extractor = TFAutoModel.from_pretrained(model_id) \n",
    "        self.AveragePool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.drop = tf.keras.layers.Dropout(.5)\n",
    "        self.out = tf.keras.layers.Dense(1 , activation = 'sigmoid')\n",
    "    def call(self , inputs) : \n",
    "        features = self.extractor(inputs)\n",
    "        X = self.AveragePool(features[0])\n",
    "        X = self.drop(X)\n",
    "        return self.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer = optimizer , loss = 'binary_crossentropy'  , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(tf_train_dataset ,validation_data = tf_eval_dataset , epochs = 2 , batch_size = 16 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "780a354d57c8e214d9717ce5db3cb79ec448e2663d29d7e7694b88eee37d2c96"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
